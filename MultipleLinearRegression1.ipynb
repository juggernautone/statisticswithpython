{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# read data into a DataFrame\n",
    "data = pd.read_csv('http://pythontrade.com/public/Data/pro2/adv.csv',index_col=0)\n",
    "data.head()\n",
    "# this is the standard import if you're using \"formula notation\" (similar to R)\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_kx_k+\\epsilon$$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption about Linear Regression Model\n",
    "- for different values of features, noises are independent and have equal variance $\\sigma$\n",
    "- noises are normal\n",
    "- means of noises are zeroes.\n",
    "\n",
    "These assumptions are important, without which, statistical properties of coefficient and prediction will not hold. That is, we still can get least square equation. $\\hat{y} = b_0 + b_1x_1 + ... + b_kx_k$, but we cannot evaluate variance and bias of the model. \n",
    "\n",
    "In many dataset, these assumptions are not true at beginning, but we have technique to transform data so that assumptions are approximately hold. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "$sales = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper+\\epsilon$\n",
    "\n",
    "Let's use Statsmodels to estimate these coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Sales</td>      <th>  R-squared:         </th> <td>   0.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   570.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 27 Nov 2015</td> <th>  Prob (F-statistic):</th> <td>1.58e-96</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:13:39</td>     <th>  Log-Likelihood:    </th> <td> -386.18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   200</td>      <th>  AIC:               </th> <td>   780.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   196</td>      <th>  BIC:               </th> <td>   793.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    2.9389</td> <td>    0.312</td> <td>    9.422</td> <td> 0.000</td> <td>    2.324     3.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TV</th>        <td>    0.0458</td> <td>    0.001</td> <td>   32.809</td> <td> 0.000</td> <td>    0.043     0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Radio</th>     <td>    0.1885</td> <td>    0.009</td> <td>   21.893</td> <td> 0.000</td> <td>    0.172     0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Newspaper</th> <td>   -0.0010</td> <td>    0.006</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.013     0.011</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>60.414</td> <th>  Durbin-Watson:     </th> <td>   2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 151.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.327</td> <th>  Prob(JB):          </th> <td>1.44e-33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.332</td> <th>  Cond. No.          </th> <td>    454.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  Sales   R-squared:                       0.897\n",
       "Model:                            OLS   Adj. R-squared:                  0.896\n",
       "Method:                 Least Squares   F-statistic:                     570.3\n",
       "Date:                Fri, 27 Nov 2015   Prob (F-statistic):           1.58e-96\n",
       "Time:                        17:13:39   Log-Likelihood:                -386.18\n",
       "No. Observations:                 200   AIC:                             780.4\n",
       "Df Residuals:                     196   BIC:                             793.6\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      2.9389      0.312      9.422      0.000         2.324     3.554\n",
       "TV             0.0458      0.001     32.809      0.000         0.043     0.049\n",
       "Radio          0.1885      0.009     21.893      0.000         0.172     0.206\n",
       "Newspaper     -0.0010      0.006     -0.177      0.860        -0.013     0.011\n",
       "==============================================================================\n",
       "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
       "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
       "Kurtosis:                       6.332   Cond. No.                         454.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a fitted model with all three features\n",
    "lm = smf.ols(formula='Sales ~ TV+Radio + Newspaper ', data=data).fit()\n",
    "\n",
    "\n",
    "# print the coefficients\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RMSE=\\sqrt{\\frac{SSE}{n-k-1}}$$\n",
    "where $k$ is # of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm3 = smf.ols(formula='Sales ~ TV+Radio+Newspaper ', data=data).fit()\n",
    "real=data['Sales']\n",
    "predict3=lm3.params[0]+lm3.params[1]*data[\"TV\"]+lm3.params[2]*data[\"Radio\"]+lm3.params[3]*data[\"Newspaper\"]\n",
    "SSE3=((real-predict3)**2).sum()\n",
    "RMSE3=np.sqrt(SSE3/(len(real)-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm2 = smf.ols(formula='Sales ~ TV+Radio ', data=data).fit()\n",
    "real=data['Sales']\n",
    "predict2=lm2.params[0]+lm2.params[1]*data[\"TV\"]+lm2.params[2]*data[\"Radio\"]\n",
    "SSE2=((real-predict2)**2).sum()\n",
    "RMSE2=np.sqrt(SSE2/(len(real)-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE3= 1.68551037341\n",
      "RMSE2= 1.68136091251\n"
     ]
    }
   ],
   "source": [
    "print \"RMSE3=\",RMSE3\n",
    "print \"RMSE2=\",RMSE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret these coefficients? For a given amount of Radio and Newspaper ad spending, an **increase of $1000 in TV ad spending** is associated with an **increase in Sales of 45.765 widgets**.\n",
    "\n",
    "A lot of the information we have been reviewing piece-by-piece is available in the model summary output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are a few key things we learn from this output?\n",
    "\n",
    "- TV and Radio have significant **p-values**, whereas Newspaper does not. Thus we reject the null hypothesis for TV and Radio (that there is no association between those features and Sales), and fail to reject the null hypothesis for Newspaper.\n",
    "- TV and Radio ad spending are both **positively associated** with Sales, whereas Newspaper ad spending is **slightly negatively associated** with Sales. (However, this is irrelevant since we have failed to reject the null hypothesis for Newspaper.)\n",
    "- This model has a higher **R-squared** (0.897) than the previous model, which means that this model provides a better fit to the data than a model that only includes TV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "How do I decide **which features to include** in a linear model? Here's one idea:\n",
    "- Try different models, and only keep predictors in the model if they have small p-values.\n",
    "- Check whether the R-squared value goes up when you add new predictors.\n",
    "\n",
    "What are the **drawbacks** to this approach?\n",
    "- Linear models rely upon a lot of **assumptions** (such as the features being independent), and if those assumptions are violated (which they usually are), R-squared and p-values are less reliable.\n",
    "- Using a p-value cutoff of 0.05 means that if you add 100 predictors to a model that are **pure noise**, 5 of them (on average) will still be counted as significant.\n",
    "- R-squared is susceptible to **overfitting**, and thus there is no guarantee that a model with a high R-squared value will generalize. Below is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89719426108289557"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only include TV and Radio in the model\n",
    "lm = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\n",
    "lm.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89721063817895208"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add Newspaper to the model (which we believe has no association with Sales)\n",
    "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\n",
    "lm.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to R-squared called **adjusted R-squared** that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html).\n",
    "\n",
    "So is there a better approach to feature selection? **Cross-validation.** It provides a more reliable estimate of out-of-sample error, and thus is a better way to choose which of your models will best **generalize** to out-of-sample data. There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models. Importantly, cross-validation can be applied to any model, whereas the methods described above only apply to linear models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
